{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 13\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Fit the logistic regression model on the scaled training data\nlr.fit(X_train_std, y_train)\n\n# Scale the test features\nX_test_std = scaler.transform(X_test)\n\n# Predict diabetes presence on the scaled test set\ny_pred = lr.predict(X_test_std)\n\n# Prints accuracy metrics and feature coefficients\nprint(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \nprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 14\n\n# Remove the feature with the lowest model coefficient\nX = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n\n# Performs a 25-75% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Scales features and fits the logistic regression model\nlr.fit(scaler.fit_transform(X_train), y_train)\n\n# Calculates the accuracy on the test set and prints coefficients\nacc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\nprint(\"{0:.1%} accuracy on test set.\".format(acc)) \nprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 15\n\n# Create the RFE with a LogisticRegression estimator and 3 features to select\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n\n# Fits the eliminator to the data\nrfe.fit(X_train, y_train)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe.ranking_)))\n\n# Print the features that are not eliminated\nprint(X.columns[rfe.support_])\n\n# Calculates the test set accuracy\nacc = accuracy_score(y_test, rfe.predict(X_test))\nprint(\"{0:.1%} accuracy on test set.\".format(acc)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 16\n\n# Perform a 75% training and 25% test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Fit the random forest model to the training data\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\n\n# Calculate the accuracy\nacc = accuracy_score(y_test, rf.predict(X_test))\n\n# Print the importances per feature\nprint(dict(zip(X.columns, rf.feature_importances_.round(2))))\n\n# Print accuracy\nprint(\"{0:.1%} accuracy on test set.\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 17\n\n# Create a mask for features importances above the threshold\nmask = rf.feature_importances_>0.15\n\n# Prints out the mask\nprint(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 18\n\n# Wrap the feature eliminator around the random forest model\nrfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 19\n\n# Set the test size to 30% to get a 70-30% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Create the Lasso model\nla = Lasso()\n\n# Fit it to the standardized training data\nla.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 20\n\n# Transform the test set with the pre-fitted scaler\nX_test_std = scaler.transform(X_test)\n\n# Calculate the coefficient of determination (R squared) on X_test_std\nr_squared = la.score(X_test_std, y_test)\nprint(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n\n# Create a list that has True values when coefficients equal 0\nzero_coef = la.coef_ == 0\n\n# Calculate how many features have a zero coefficient\nn_ignored = sum(zero_coef)\nprint(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 21\n\n# Find the highest alpha value with R-squared above 98%\nla = Lasso(alpha=0.1, random_state=0)\n\n# Fits the model and calculates performance stats\nla.fit(X_train_std, y_train)\nr_squared = la.score(X_test_std, y_test)\nn_ignored_features = sum(la.coef_ == 0)\n\n# Print peformance stats \nprint(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\nprint(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 22\n\nfrom sklearn.linear_model import LassoCV\n\n# Create and fit the LassoCV model on the training set\nlcv = LassoCV()\nlcv.fit(X_train,y_train)\nprint('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n\n# Calculate R squared on the test set\nr_squared = lcv.score(X_test,y_test)\nprint('The model explains {0:.1%} of the test set variance'.format(r_squared))\n\n# Create a mask for coefficients not equal to zero\nlcv_mask = lcv.coef_!=0\nprint('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 23\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\nrfe_gb = RFE(estimator=GradientBoostingRegressor(), \n             n_features_to_select=10, step=3, verbose=1)\nrfe_gb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 24\n\n# Sum the votes of the three models\nvotes = np.sum([lcv_mask,rf_mask,gb_mask],axis=0)\nprint(votes)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}